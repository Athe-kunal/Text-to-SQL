{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tables for US:   0%|          | 0/24 [00:49<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib.request\n",
    "import ssl\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "from sqlalchemy import inspect\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text \n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "html_link = \"https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html\"\n",
    "\n",
    "with urllib.request.urlopen(html_link) as url:\n",
    "    s = url.read()\n",
    "    # I'm guessing this would output the html source code ?\n",
    "    soup = BeautifulSoup(s,\"lxml\")\n",
    "\n",
    "html_table = soup.find_all(\"table\")\n",
    "req_table = html_table[1]\n",
    "hrefs_list = req_table.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_href = {\"US\":[],\"Europe\":[],\"Japan\":[],\"AUS_NZ_CANADA\":[],\"Emerging\":[],\"China\":[],\"India\":[],\"Global\":[]}\n",
    "\n",
    "for i in hrefs_list:\n",
    "    name = i.get_text().strip()\n",
    "    try:\n",
    "        href_attr = i['href']\n",
    "        if href_attr.endswith('.xls'):\n",
    "            if \"US\" in name:\n",
    "                req_href[\"US\"].append(href_attr)\n",
    "            elif \"Europe\" in name:\n",
    "                req_href[\"Europe\"].append(href_attr)\n",
    "            elif \"Japan\" in name:\n",
    "                req_href[\"Japan\"].append(href_attr)\n",
    "            elif \"Aus\" in name:\n",
    "                req_href['AUS_NZ_CANADA'].append(href_attr)\n",
    "            elif \"Emerging\" in name:\n",
    "                req_href['Emerging'].append(href_attr)\n",
    "            elif \"China\" in name:\n",
    "                req_href['China'].append(href_attr)\n",
    "            elif \"India\" in name:\n",
    "                req_href['India'].append(href_attr)\n",
    "            elif \"Global\" in name: \n",
    "                req_href['Global'].append(href_attr)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import os\n",
    "os.makedirs(\"DATA\",exist_ok=True)\n",
    "for country,excel_files in req_href.items():\n",
    "    country_path = os.path.join(\"DATA\",country) \n",
    "    os.makedirs(country_path,exist_ok=True)\n",
    "    for file in excel_files:\n",
    "        file_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        full_file_name = os.path.join(country_path,f\"{file_name}.xls\")\n",
    "        resp = requests.get(file,verify=False)\n",
    "        output = open(full_file_name, 'wb')\n",
    "        output.write(resp.content)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR Emerging WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Europe WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Global WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR AUS_NZ_CANADA WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR China WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Japan WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR US WE HAVE DIRECTORY LEN = 24 and ACTUAL LEN = 24\n",
      "FOR India WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n"
     ]
    }
   ],
   "source": [
    "for country in os.listdir(\"DATA\"):\n",
    "    dir_len = len(os.listdir(os.path.join(\"DATA\",country)))\n",
    "    country_len = len(req_href[country])\n",
    "    print(f'FOR {country} WE HAVE DIRECTORY LEN = {dir_len} and ACTUAL LEN = {country_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emerging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 86.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 94.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 102.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUS_NZ_CANADA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 104.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 103.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 66.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 102.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 100.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "dir = \"DATA2\"\n",
    "processed_dir = \"Processed Data\"\n",
    "all_infos_dict = []\n",
    "os.makedirs(processed_dir,exist_ok=True)\n",
    "for country in os.listdir(dir):\n",
    "    print(country)\n",
    "    file_name = os.path.join(dir,country)\n",
    "    os.makedirs(os.path.join(processed_dir,country),exist_ok=True)\n",
    "    os.makedirs(file_name,exist_ok=True)\n",
    "    for excel_file in tqdm(os.listdir(file_name)):\n",
    "        full_file_name = os.path.join(file_name,excel_file)\n",
    "        xls = pd.ExcelFile(full_file_name)\n",
    "        sns = xls.sheet_names\n",
    "        for sheet_name in sns:\n",
    "            if \"Var\" in sheet_name or \"var\" in sheet_name:\n",
    "                info_df = xls.parse(sheet_name)\n",
    "                info_df = xls.parse(sheet_name)\n",
    "                info_df.dropna(how=\"all\",inplace=True)\n",
    "                info_dict = {}\n",
    "                for cols in info_df.columns:\n",
    "                    if \"End\" not in cols and 'Unnamed' not in cols:\n",
    "                        info_dict['Summary'] = cols\n",
    "                info_dict['Vars'] = info_df.values[1:].tolist()\n",
    "                all_infos_dict.append(info_dict)\n",
    "            elif \"Industry\" in sheet_name or \"industry\" in sheet_name:\n",
    "                data_df = xls.parse(sheet_name)\n",
    "        try:\n",
    "            data_df.dropna(axis=1,thresh=5,inplace=True)\n",
    "            data_df.dropna(inplace=True)\n",
    "            new_header = data_df.iloc[0] #grab the first row for the header\n",
    "        except:\n",
    "            print(full_file_name)\n",
    "            print(data_df)\n",
    "        data_df = data_df[1:] #take the data less the header row\n",
    "        data_df.reset_index(inplace=True,drop=True)\n",
    "        new_header = [sanitize_column_name(str(col)) for col in new_header]\n",
    "        data_df.columns = new_header #set the header row as the df header\n",
    "        save_name = full_file_name.split(\".\")[0].split(\"/\")[-1]\n",
    "        save_file_path = os.path.join(os.path.join(processed_dir,country),save_name)\n",
    "        data_df.to_csv(save_file_path+\".csv\",index=False)\n",
    "        # with open(save_file_path+\".json\", \"w\") as outfile: \n",
    "        #     json.dump(info_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Summary': 'Measures of accounting returns, to all claim holders (and from operations)',\n",
       " 'Vars': [['Number of firms',\n",
       "   'Number of firms in the industry grouping.',\n",
       "   'Law of large numbers?'],\n",
       "  ['R&D Capitalized',\n",
       "   'My estimate of R&D capitalization, based upon a 5-year straight line amortization period, aggregated across firms in the group',\n",
       "   'Capitalized value of R&D gets added on to book equity and to invested capital'],\n",
       "  ['Capitalized R&D as percent of invested capital',\n",
       "   'My R&D capitalization estimate, as a percent of invested capital including that number, based upon aggregated values across firms',\n",
       "   'Magnitude of investment in R&D, relative to investment in more traditional capital expenditures.'],\n",
       "  ['R&D -LTM',\n",
       "   'Aggregated R&D expenses across the last twelve months, across companies in the group.',\n",
       "   'Spending on R&D in most recent year'],\n",
       "  ['R&D: Years minus 1 to minus 5',\n",
       "   'Aggregated R&D expenses for each of the previous five years, across companies in the group.',\n",
       "   'Sepnding on R&D over last five years']]}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_infos_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry_Name</th>\n",
       "      <th>Number_of_Firms</th>\n",
       "      <th>Capital_Expenditures_(US_$_millions)</th>\n",
       "      <th>Depreciation_&amp;_Amort_((US_$_millions)</th>\n",
       "      <th>Cap_Ex/Deprecn</th>\n",
       "      <th>Acquisitions_(US_$_millions)</th>\n",
       "      <th>Net_R&amp;D_(US_$_millions)</th>\n",
       "      <th>Net_Cap_Ex/Sales</th>\n",
       "      <th>Net_Cap_Ex/_EBIT_(1-t)</th>\n",
       "      <th>Sales/_Invested_Capital_(LTM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Advertising</td>\n",
       "      <td>57</td>\n",
       "      <td>775.729</td>\n",
       "      <td>1887.338</td>\n",
       "      <td>0.411018</td>\n",
       "      <td>322.559</td>\n",
       "      <td>75.0800</td>\n",
       "      <td>-0.016886</td>\n",
       "      <td>-0.218120</td>\n",
       "      <td>3.283403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aerospace/Defense</td>\n",
       "      <td>70</td>\n",
       "      <td>10982.128</td>\n",
       "      <td>13311.598</td>\n",
       "      <td>0.825004</td>\n",
       "      <td>10344.750</td>\n",
       "      <td>830.4634</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.318077</td>\n",
       "      <td>1.984340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air Transport</td>\n",
       "      <td>25</td>\n",
       "      <td>25559.725</td>\n",
       "      <td>10609.948</td>\n",
       "      <td>2.409034</td>\n",
       "      <td>368.210</td>\n",
       "      <td>73.5486</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>1.355173</td>\n",
       "      <td>1.777320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>38</td>\n",
       "      <td>1730.981</td>\n",
       "      <td>1386.480</td>\n",
       "      <td>1.248472</td>\n",
       "      <td>38.739</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.072557</td>\n",
       "      <td>1.773076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auto &amp; Truck</td>\n",
       "      <td>34</td>\n",
       "      <td>29899.486</td>\n",
       "      <td>18677.668</td>\n",
       "      <td>1.600815</td>\n",
       "      <td>193.220</td>\n",
       "      <td>983.0696</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.675918</td>\n",
       "      <td>1.048732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Industry_Name  Number_of_Firms  Capital_Expenditures_(US_$_millions)  \\\n",
       "0        Advertising               57                               775.729   \n",
       "1  Aerospace/Defense               70                             10982.128   \n",
       "2      Air Transport               25                             25559.725   \n",
       "3            Apparel               38                              1730.981   \n",
       "4       Auto & Truck               34                             29899.486   \n",
       "\n",
       "   Depreciation_&_Amort_((US_$_millions)  Cap_Ex/Deprecn  \\\n",
       "0                               1887.338        0.411018   \n",
       "1                              13311.598        0.825004   \n",
       "2                              10609.948        2.409034   \n",
       "3                               1386.480        1.248472   \n",
       "4                              18677.668        1.600815   \n",
       "\n",
       "   Acquisitions_(US_$_millions)  Net_R&D_(US_$_millions)  Net_Cap_Ex/Sales  \\\n",
       "0                       322.559                  75.0800         -0.016886   \n",
       "1                     10344.750                 830.4634          0.022829   \n",
       "2                       368.210                  73.5486          0.067203   \n",
       "3                        38.739                   0.9474          0.005365   \n",
       "4                       193.220                 983.0696          0.026577   \n",
       "\n",
       "   Net_Cap_Ex/_EBIT_(1-t)  Sales/_Invested_Capital_(LTM)  \n",
       "0               -0.218120                       3.283403  \n",
       "1                0.318077                       1.984340  \n",
       "2                1.355173                       1.777320  \n",
       "3                0.072557                       1.773076  \n",
       "4                0.675918                       1.048732  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Processed Data/US/capex.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD TABLE NAMES AND METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',Industry_Name,Number_of_Firms,Capital_Expenditures_(US_$_millions),Depreciation_&_Amort_((US_$_millions),Cap_Ex/Deprecn,Acquisitions_(US_$_millions),Net_R&D_(US_$_millions),Net_Cap_Ex/Sales,Net_Cap_Ex/_EBIT_(1-t),Sales/_Invested_Capital_(LTM)\\n0,Advertising,57,775.729,1887.338,0.4110175283918408,322.559,75.07999999999993,-0.0168860862168837,-0.2181203933865563,3.283403041333076\\n1,Aerospace/Defense,70,10982.128,13311.597999999998,0.8250044810547916,10344.749999999998,830.4633999999933,0.0228292321038578,0.318077061754813,1.9843395804666923\\n2,Air Transport,25,25559.725,10609.948,2.4090339556800844,368.21,73.5486000000019,0.0672027546276203,1.3551729988499104,1.7773199625336142\\n3,Apparel,38,1730.9809999999998,1386.4799999999996,1.2484716692631703,38.739,0.9473999999991064,0.0053650838666078,0.0725567415076527,1.7730762352050575\\n4,Auto & Truck,34,29899.486,18677.668,1.6008147269776931,193.22,983.0695999999988,0.0265766732899557,0.6759175844836917,1.048731678622114\\n5,Auto Parts,39,3565.4210000000007,2034.471,1.7525051966825778,1061.7600000000002,454.9352000000008,0.0316936036463534,0.8398185079860373,2.004790963758861\\n6,Beverage (Alcoholic),19,2244.8160000000007,864.0629999999999,2.59797723082692,1370.0,0.0,0.0939030584900288,0.6367443614850201,0.8968231153352131\\n7,Beverage (Soft),29,7926.687000000001,4543.389,1.7446639501922463,674.6669999999999,7.705600000001141,0.0238699281532688,0.1517545513863501,1.6931990505110932\\n8,Broadcasting,22,1837.548,3387.6400000000003,0.5424271764414165,43.856,0.2399999999997817,-0.0206121524427381,-0.2104150803584717,1.1159893806472496\\n9,Brokerage & Investment Banking,27,7783.945999999999,4646.382,1.675270350134793,876.0999999999999,-189.38079999999945,0.0167661627913481,3.4298253908840737,0.2794992243703733\\n'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=250)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "class SQLTableMetadata(dspy.Signature):\n",
    "    \"\"\"Give a suitable table name and description about the given table\"\"\"\n",
    "    pandas_dataframe_str = dspy.InputField(desc=\"First 10 rows of a pandas dataframe delimited by newline character\")\n",
    "    table_name = dspy.OutputField(desc=\"suitable table name\")\n",
    "    table_summary = dspy.OutputField(desc=\"a summary about the table\")\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(SQLTableMetadata)\n",
    "    \n",
    "    def forward(self, pandas_dataframe_str):\n",
    "        return self.prog(pandas_dataframe_str=pandas_dataframe_str)\n",
    "\n",
    "cot = CoT()\n",
    "\n",
    "# cot(pandas_dataframe_str = df.head(10).to_csv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [08:18<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [12:00<00:00, 12.43s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_dir = \"Processed Data\"\n",
    "\n",
    "for country in os.listdir(processed_dir):\n",
    "    country_folder = os.path.join(processed_dir,country)\n",
    "    print(f\"{country}\")\n",
    "    for files in tqdm(os.listdir(country_folder)):\n",
    "        # print(files)\n",
    "        if files.endswith(\".csv\"):\n",
    "            file_name = files.split(\".\")[0]\n",
    "            csv_file_path = os.path.join(country_folder,files)\n",
    "            df = pd.read_csv(csv_file_path,index_col=False)\n",
    "            json_file_path = os.path.join(country_folder,f\"{file_name}.json\")\n",
    "            with open(json_file_path,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            if 'table_name' in data and 'table_summary' in data:\n",
    "                # if data['table_name'] == \"\" or data['table_summary'] == \"\":\n",
    "                if data['table_summary'] == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "            table_preds = cot(pandas_dataframe_str = df.head(10).to_csv())\n",
    "            data['table_name'] = table_preds.table_name\n",
    "            data['table_summary'] = table_preds.table_summary\n",
    "            with open(json_file_path,'w') as f:\n",
    "                json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT TASKS\n",
    "1. Build database with each region for each table\n",
    "2. Embed the table summary and table SCHEMA. Also, embed the table rows\n",
    "3. Retrieval at table level and embed the rows to retrieve relevant rows from the retrieved schema of table\n",
    "4. Final LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATABASE FOR US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir  = \"Processed Data\"\n",
    "def sqlalchemy_engine(region:str):\n",
    "    \"\"\"Create a SQLAlchemy engine for the given region\"\"\"\n",
    "    assert region in os.listdir(processed_dir), f\"{region} is not a valid region from {os.listdir(processed_dir)}\"\n",
    "    engine = create_engine(f\"sqlite:///{region}.db\")\n",
    "    metadata_obj = MetaData()\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    dfs = []\n",
    "    for dataframes_path in os.listdir(region_path):\n",
    "        if dataframes_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(region_path,dataframes_path),index_col=False)\n",
    "            dfs.append((dataframes_path,df))\n",
    "    pbar = tqdm(total=len(dfs),desc=f\"Creating tables for {region}\")\n",
    "    for _, df_table_name in enumerate(dfs):\n",
    "        table_name = df_table_name[0]\n",
    "        table_name = table_name.split(\".\")[0]\n",
    "        df = df_table_name[1]\n",
    "        # print(f\"Creating table: {table_name}\")\n",
    "        create_table_from_dataframe(df,table_name, engine, metadata_obj)\n",
    "        # print(f\"Done creating table for: {table_name}\")\n",
    "        pbar.update(1)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tables for US: 100%|██████████| 24/24 [00:00<00:00, 29.65it/s]\n",
      "Creating tables for India: 100%|██████████| 29/29 [00:00<00:00, 31.34it/s]\n",
      "Creating tables for China: 100%|██████████| 29/29 [00:01<00:00, 28.19it/s]\n",
      "Creating tables for Europe: 100%|██████████| 29/29 [00:00<00:00, 30.01it/s]\n",
      "Creating tables for Global: 100%|██████████| 29/29 [00:01<00:00, 26.46it/s]\n",
      "Creating tables for AUS_NZ_CANADA: 100%|██████████| 29/29 [00:00<00:00, 31.96it/s]\n",
      "Creating tables for Japan: 100%|██████████| 29/29 [00:00<00:00, 32.00it/s]\n",
      "Creating tables for Emerging: 100%|██████████| 29/29 [00:00<00:00, 30.77it/s]\n"
     ]
    }
   ],
   "source": [
    "us_engine = sqlalchemy_engine(\"US\")\n",
    "india_engine = sqlalchemy_engine(\"India\")\n",
    "china_engine = sqlalchemy_engine(\"China\")\n",
    "europe_engine = sqlalchemy_engine(\"Europe\")\n",
    "global_engine = sqlalchemy_engine(\"Global\")\n",
    "aus_nz_canada_engine = sqlalchemy_engine(\"AUS_NZ_CANADA\")\n",
    "japan_engine = sqlalchemy_engine(\"Japan\")\n",
    "emerging_engine = sqlalchemy_engine(\"Emerging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_infos(sql_engine:sqlalchemy.engine.base.Engine,region:str):\n",
    "    \"\"\"Get all tables in the database\"\"\"\n",
    "    inspector = inspect(sql_engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "    table_infos_dict = {tb: [] for tb in table_names}\n",
    "    for tb in table_names:\n",
    "        column_dict = inspector.get_columns(tb)\n",
    "        schema_str = \"\"\n",
    "        primary_keys = []\n",
    "        for col in column_dict:\n",
    "            schema_str += f\"{col['name']} ({col['type']}), \"\n",
    "            if col[\"primary_key\"] not in primary_keys:\n",
    "                primary_keys.append(col[\"name\"])\n",
    "        with open(os.path.join(processed_dir,region,f\"{tb}.json\")) as f:\n",
    "            table_info = json.loads(f.read())\n",
    "        table_infos_dict[tb] = [\n",
    "            {\n",
    "                \"table_info\": f\"Table {tb} has columns: {schema_str[:-2]}\",\n",
    "                \"table_summary\": f'{table_info[\"Summary\"]}. {table_info[\"table_summary\"]}',\n",
    "            }\n",
    "        ]\n",
    "    return table_infos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_tb_dict = get_table_infos(us_engine,\"US\")\n",
    "india_tb_dict = get_table_infos(india_engine,\"India\")\n",
    "china_tb_dict = get_table_infos(china_engine,\"China\")\n",
    "europe_tb_dict = get_table_infos(europe_engine,\"Europe\")\n",
    "global_tb_dict = get_table_infos(global_engine,\"Global\")\n",
    "aus_nz_canada_tb_dict = get_table_infos(aus_nz_canada_engine,\"AUS_NZ_CANADA\")\n",
    "japan_tb_dict = get_table_infos(japan_engine,\"Japan\")\n",
    "emerging_tb_dict = get_table_infos(emerging_engine,\"Emerging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS\n",
    "\n",
    "1. Embed the table summary and table SCHEMA to get the table that the user is looking for\n",
    "2. Embed the table rows for each table, so as to get relevant rows from the retrieved table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBED THE TABLE SUMMARY AND TABLE SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "\n",
    "load_dotenv(override=True)\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.environ['OPENAI_API_KEY'],\n",
    "                model_name=\"text-embedding-3-small\")\n",
    "# EMBEDDING_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# emb_fn = embedding_functions.HuggingFaceEmbeddingFunction(model_name=EMBEDDING_MODEL,api_key=os.environ[\"HF_API_KEY\"])\n",
    "def embed_table_info(region:str,tb_dict):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "\n",
    "    table_collection = client.get_or_create_collection(name=\"table\",embedding_function=emb_fn)\n",
    "\n",
    "    table_docs = []\n",
    "    table_metadata = []\n",
    "\n",
    "\n",
    "    for table_name,table_data in tb_dict.items():\n",
    "        table_docs.append(table_data[0]['table_info'] + \". \" + table_data[0]['table_summary'])\n",
    "        table_metadata.append({\"table_name\":table_name,'table_metadata':table_data[0]['table_info']})\n",
    "    table_ids = [f\"id{i}\" for i in range(len(table_docs))]\n",
    "    assert len(table_docs) == len(table_metadata)\n",
    "\n",
    "    batches = create_batches(api=client,ids=table_ids, documents=table_docs, metadatas=table_metadata)\n",
    "    for batch in tqdm(batches,desc=\"Embedding table info\"):\n",
    "        table_collection.add(ids=batch[0],\n",
    "                    documents=batch[3],\n",
    "                    metadatas=batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"US\"\n",
    "\n",
    "def embed_rows(region:str,batch_size:int=24):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "\n",
    "    rows_collection = client.get_or_create_collection(name=\"rows\",embedding_function=emb_fn)\n",
    "\n",
    "    rows_docs = []\n",
    "    rows_metadata = []\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    for df_path in os.listdir(region_path):\n",
    "        df_full_path = os.path.join(region_path,df_path)\n",
    "        df = pd.read_csv(df_full_path,index_col=False)\n",
    "        for idx,row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for rv in row.values:\n",
    "                row_str+=str(rv) + \", \"\n",
    "                row_str = row_str.replace('\"',\"\")\n",
    "                row_str = row_str.replace(\"'\",'\"')\n",
    "            rows_docs.append(row_str[:-2])\n",
    "            rows_metadata.append({\"table_name\":df_path.split(\".\")[0],\"region\":region,\"index\":idx})\n",
    "    row_ids = [f\"id{i}\" for i in range(len(rows_docs))]\n",
    "    # print(len(rows_docs),len(rows_metadata))\n",
    "    assert len(rows_docs) == len(rows_metadata) == len(row_ids)\n",
    "\n",
    "    for start in tqdm(range(0,len(rows_docs),batch_size),desc=\"Embedding rows\"):\n",
    "        end = min(start+batch_size,len(rows_docs))\n",
    "        batch_ids = row_ids[start:end]\n",
    "        batch_rows = rows_docs[start:end]\n",
    "        batch_metadatas = rows_metadata[start:end]\n",
    "        rows_collection.add(ids=batch_ids,\n",
    "                    documents=batch_rows,\n",
    "                    metadatas=batch_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Embedding rows: 100%|██████████| 5/5 [01:09<00:00, 13.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"US\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:20<00:00, 20.73s/it]\n",
      "Embedding rows: 100%|██████████| 3/3 [00:50<00:00, 16.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"India\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Embedding rows: 100%|██████████| 3/3 [00:49<00:00, 16.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"China\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:20<00:00, 20.69s/it]\n",
      "Embedding rows: 100%|██████████| 3/3 [01:09<00:00, 23.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"Europe\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:20<00:00, 20.66s/it]\n",
      "Embedding rows: 100%|██████████| 3/3 [00:49<00:00, 16.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"Global\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:20<00:00, 20.73s/it]\n",
      "Embedding rows: 100%|██████████| 3/3 [01:13<00:00, 24.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"Emerging\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Embedding rows: 100%|██████████| 3/3 [01:09<00:00, 23.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"Japan\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info: 100%|██████████| 1/1 [00:20<00:00, 20.70s/it]\n",
      "Embedding rows: 100%|██████████| 3/3 [00:48<00:00, 16.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# region = \"AUS_NZ_CANADA\"\n",
    "# embed_table_info(region,us_tb_dict)\n",
    "# embed_rows(region,1104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT-TO-SQL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"US\"\n",
    "\n",
    "db_dict = {\n",
    "    \"US\":us_engine,\n",
    "    \"India\":india_engine,\n",
    "    \"China\":china_engine,\n",
    "    \"Europe\":europe_engine,\n",
    "    \"Global\":global_engine,\n",
    "    \"AUS_NZ_CANADA\":aus_nz_canada_engine,\n",
    "    \"Japan\":japan_engine,\n",
    "    \"Emerging\":emerging_engine,\n",
    "}\n",
    "\n",
    "def get_collections_db(region:str):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "    table_collection = client.get_collection(name=\"table\",embedding_function=emb_fn)\n",
    "    row_collection = client.get_collection(name=\"rows\",embedding_function=emb_fn)\n",
    "    return [db_dict[region],table_collection,row_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_collection_dict = {\n",
    "    \"US\":get_collections_db(\"US\"),\n",
    "    \"India\":get_collections_db(\"India\"),\n",
    "    \"China\":get_collections_db(\"China\"),\n",
    "    \"Europe\":get_collections_db(\"Europe\"),\n",
    "    \"Global\":get_collections_db(\"Global\"),\n",
    "    \"AUS_NZ_CANADA\":get_collections_db(\"AUS_NZ_CANADA\"),\n",
    "    \"Japan\":get_collections_db(\"Japan\"),\n",
    "    \"Emerging\":get_collections_db(\"Emerging\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Engine(sqlite:///US.db), Collection(name=table), Collection(name=rows)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection_dict['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 1: Table histgr has columns: Industry_Name (VARCHAR), Number_of_Firms (INTEGER), CAGR_in_Net_Income_Last_5_years (INTEGER), CAGR_in_Revenues_Last_5_years (INTEGER), Expected_Growth_in_Revenues_Next_2_years (INTEGER), Expected_growth_in_Revenues_Next_5_years (INTEGER), Expected_Growth_in_EPS_Next_5_years (INTEGER)\n",
      "\tRow 1: Heathcare Information and Technology, 128, 0.2291124999999999, 0.1966670689655173, 0.1482884810126581, 0.1667594853751504, 0.1027218749999999\n",
      "\tRow 2: Hospitals/Healthcare Facilities, 32, 0.0409985714285714, 0.181068947368421, 0.0413333333333333, 0.0408655475254498, 0.1064555555555555\n",
      "\tRow 3: Healthcare Support Services, 119, 0.0625710344827586, 0.1242187719298245, 0.0938797014925373, 0.1106991073675205, 0.1118653846153846\n",
      "\n",
      "\n",
      "Schema 2: Table wacc has columns: Industry_Name (VARCHAR), Number_of_Firms (INTEGER), Beta (INTEGER), Cost_of_Equity (INTEGER), E_D_E_ (INTEGER), Std_Dev_in_Stock (INTEGER), Cost_of_Debt (INTEGER), Tax_Rate (INTEGER), After_tax_Cost_of_Debt (INTEGER), D_D_E_ (INTEGER), Cost_of_Capital (INTEGER), Cost_of_Capital_Local_Currency_ (INTEGER)\n",
      "\tRow 1: Heathcare Information and Technology, 128, 1.2747542568245518, 0.0974386958139293, 0.8615496523681622, 0.5415316524692178, 0.053524, 0.0311109029990876, 0.040143, 0.1384503476318378, 0.0895060868106828, 0.0895060868106827\n",
      "\tRow 2: Healthcare Support Services, 119, 1.0326229061406076, 0.0863006536824679, 0.7882476948159491, 0.495257334489641, 0.050855, 0.0808427903603178, 0.03814125, 0.2117523051840508, 0.076102788936416, 0.076102788936416\n",
      "\tRow 3: Hospitals/Healthcare Facilities, 32, 0.879576324903561, 0.0792605109455638, 0.5563496280559845, 0.4632806494431785, 0.050855, 0.0685657680171947, 0.03814125, 0.4436503719440154, 0.0610179355330013, 0.0610179355330013\n",
      "\n",
      "\n",
      "Schema 3: Table wcdata has columns: Industry_Name (VARCHAR), Number_of_firms (INTEGER), Acc_Rec_Sales (INTEGER), Inventory_Sales (INTEGER), Acc_Pay_Sales (INTEGER), Non_cash_WC_Sales (INTEGER)\n",
      "\tRow 1: Heathcare Information and Technology, 128, 0.1905978772772344, 0.1033615227735673, 0.0699539122720788, 0.2294587527373559\n",
      "\tRow 2: Healthcare Support Services, 119, 0.0673778102069709, 0.0378014818570354, 0.1476884404540714, -0.0652541374156519\n",
      "\tRow 3: Hospitals/Healthcare Facilities, 32, 0.1456206709191285, 0.0221993047034708, 0.0656146602309952, 0.1117341333415825\n",
      "\n",
      "\n",
      "Schema 4: Table DollarUS has columns: Industry_Name (VARCHAR), Number_of_firms (INTEGER), Average_Company_Age_years_ (INTEGER), Market_Cap_millions_ (INTEGER), Book_Equity_millions_ (INTEGER), Enteprise_Value_millions_ (INTEGER), Invested_Capital_millions_ (INTEGER), Total_Debt_including_leases_millions_ (INTEGER), Revenues_millions_ (INTEGER), Gross_Profit_millions_ (INTEGER), EBITDA_millions_ (INTEGER), EBIT_Operating_Income_millions_ (INTEGER), Net_Income_millions_ (INTEGER)\n",
      "\tRow 1: Heathcare Information and Technology, 128, 22.776785714285715, 738408.3160000001, 171575.33200000005, 821060.925284837, 123028.59595086362, 118661.63228483696, 155242.41099999996, 74003.267, 34169.073000000004, 21790.95854303259, 8883.584000000004\n",
      "\tRow 2: Hospitals/Healthcare Facilities, 32, 25.23333333333333, 122470.973, 11469.825000000004, 216360.12943388568, 87395.24268778635, 97662.13543388566, 138712.903, 49920.916000000005, 24233.965, 16018.714313222865, 7107.354999999999\n",
      "\tRow 3: Household Products, 93, 42.35365853658536, 650300.6159999999, 88515.31300000002, 735763.2897524642, 99815.21822251934, 107660.48975246462, 215472.30899999995, 104939.353, 45619.29299999999, 36587.74384950707, 23883.999000000007\n",
      "\n",
      "\n",
      "Schema 5: Table inshold has columns: Industry_Name (VARCHAR), Number_of_Firms (INTEGER), CEO_Holding (INTEGER), Corporate_Holdings (INTEGER), Institutional_Holdings (INTEGER), Insider_Holdings (INTEGER)\n",
      "\tRow 1: Heathcare Information and Technology, 128, 0.04988, 0.0408264285714285, 0.470247596153846, 0.1338599999999999\n",
      "\tRow 2: Hospitals/Healthcare Facilities, 32, 0.015661304347826, 0.0204138095238095, 0.5200318181818182, 0.14894\n",
      "\tRow 3: Healthcare Support Services, 119, 0.0664136842105263, 0.0577109523809523, 0.4872983157894737, 0.1775136274509803\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "load_dotenv(override=True)\n",
    "text_to_sql = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=500)\n",
    "# text_to_sql = dspy.Google(model=\"models/gemini-1.5-pro\", max_output_tokens=500)\n",
    "\n",
    "class TextToSQLAnswer(dspy.Signature):\n",
    "    \"\"\"Convert natural language text to SQL using suitable schema(s) from multiple schema choices\"\"\"\n",
    "\n",
    "    question:str = dspy.InputField(desc=\"natural language input which will be converted to SQL\")\n",
    "    relevant_table_schemas_rows:str = dspy.InputField(desc=\"Schemas of the multiple possible tables alongwith relevant rows from the table (values in the same order as columns above)\")\n",
    "    sql:str = dspy.OutputField(desc=\"SQL output with correct column names using suitable schema(s)\")\n",
    "\n",
    "# class SQLToAnswer(dspy.Signature):\n",
    "dspy.settings.configure(lm=text_to_sql)\n",
    "class TextToSQLQueryModule(dspy.Module):\n",
    "    def __init__(self,region:str,use_cot:bool=True):\n",
    "        super().__init__()\n",
    "        self.region = region\n",
    "        db,table_collection,row_collection = db_collection_dict[region]\n",
    "        self.table_collection = table_collection\n",
    "        self.use_cot = use_cot\n",
    "        self.db = db\n",
    "        self.row_collection = row_collection\n",
    "        if self.use_cot == True:\n",
    "            self.sqlAnswer = dspy.ChainOfThought(TextToSQLAnswer)\n",
    "        else:\n",
    "            self.sqlAnswer = dspy.Predict(TextToSQLAnswer)\n",
    "        \n",
    "    def forward(self,question:str):\n",
    "        question_emb = emb_fn.embed_with_retries(question)\n",
    "        docs = self.table_collection.query(\n",
    "            # query_texts=question,\n",
    "            query_embeddings = question_emb,\n",
    "            n_results = 5\n",
    "        )\n",
    "        # relevant_table_schemas = \"\"\n",
    "        relevant_rows_schemas = \"\"\n",
    "        existing_table_names = []\n",
    "        for table_idx,metadata_name in enumerate(docs['metadatas'][0]):\n",
    "            table_metadata = metadata_name['table_metadata']\n",
    "            # relevant_table_schemas += f'Schema {idx+1}: {table_metadata}\\n'\n",
    "            table_name = metadata_name['table_name']\n",
    "            if table_name in existing_table_names: \n",
    "                continue\n",
    "            existing_table_names.append(table_name)\n",
    "            # print(table_name)\n",
    "            rows = self.row_collection.query(\n",
    "                # query_texts=question,\n",
    "                query_embeddings = question_emb,\n",
    "                n_results = 3,\n",
    "                where = {\"table_name\":table_name}\n",
    "            )\n",
    "            relevant_rows_schemas += f'Schema {table_idx+1}: {table_metadata}\\n'\n",
    "            for row_idx,row in enumerate(rows['documents'][0]):\n",
    "                relevant_rows_schemas += f'\\tRow {row_idx+1}: {row}\\n'\n",
    "            relevant_rows_schemas += '\\n\\n'\n",
    "        print(relevant_rows_schemas)\n",
    "        sql_query = self.sqlAnswer(question=question,relevant_table_schemas_rows=relevant_rows_schemas)\n",
    "        return sql_query\n",
    "\n",
    "tsql = TextToSQLQueryModule(\"US\")\n",
    "sq = tsql(question=\"How does the beta of utility companies compare to Packaging industry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale='produce the sql. We first need to gather data on the beta of utility companies and packaging industry. We can do this by joining the tables histgr and wacc on the column Industry_Name. Then, we need to compare the beta values for these two industries. We can do this by selecting the beta column from the joined table and using a WHERE clause to filter for the two industries. Finally, we can use a comparison operator to compare the beta values and return the result.',\n",
       "    sql=\"SELECT wacc.Beta FROM histgr INNER JOIN wacc ON histgr.Industry_Name = wacc.Industry_Name WHERE histgr.Industry_Name = 'Utility Companies' OR histgr.Industry_Name = 'Packaging Industry' AND wacc.Beta > histgr.Beta\"\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale='produce the sql. We first need to identify the table that contains information about the beta of different industries. Looking at the given table schemas, we can see that Schema 2 (wacc) contains the column for beta. We then need to filter the rows based on the industry name. In this case, we are looking for the industry \"Utility\". Therefore, we need to filter the rows where the Industry_Name column has the value \"Utility (Water)\". Finally, we need to select the beta column from the filtered rows. This will give us the beta of Utility industries.',\n",
       "    sql=\"SELECT Beta FROM wacc WHERE Industry_Name = 'Utility (Water)'\"\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7139634340794939,), (0.7139634340794939,), (0.7139634340794939,), (0.7139634340794939,), (0.7139634340794939,), (0.7139634340794939,)]\n"
     ]
    }
   ],
   "source": [
    "with us_engine.connect() as conn:\n",
    "    result = conn.execute(text(sq.sql))\n",
    "    print(result.fetchall())\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such column: histgr.Beta\n[SQL: SELECT wacc.Beta FROM histgr INNER JOIN wacc ON histgr.Industry_Name = wacc.Industry_Name WHERE histgr.Industry_Name = 'Utility Companies' OR histgr.Industry_Name = 'Packaging Industry' AND wacc.Beta > histgr.Beta]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1971\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:919\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 919\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: histgr.Beta",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m us_engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43msq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mfetchall())\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1422\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:514\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1644\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1632\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1636\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1637\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1638\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1643\u001b[0m )\n\u001b[0;32m-> 1644\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1658\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1659\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         ret,\n\u001b[1;32m   1664\u001b[0m     )\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1850\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1990\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2357\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2356\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2359\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1971\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1969\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1978\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1982\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1983\u001b[0m     )\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:919\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 919\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such column: histgr.Beta\n[SQL: SELECT wacc.Beta FROM histgr INNER JOIN wacc ON histgr.Industry_Name = wacc.Industry_Name WHERE histgr.Industry_Name = 'Utility Companies' OR histgr.Industry_Name = 'Packaging Industry' AND wacc.Beta > histgr.Beta]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "with us_engine.connect() as conn:\n",
    "    result = conn.execute(text(sq.sql))\n",
    "    print(result.fetchall())\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
