{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib.request\n",
    "import ssl\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "from sqlalchemy import inspect\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "html_link = \"https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html\"\n",
    "\n",
    "with urllib.request.urlopen(html_link) as url:\n",
    "    s = url.read()\n",
    "    # I'm guessing this would output the html source code ?\n",
    "    soup = BeautifulSoup(s,\"lxml\")\n",
    "\n",
    "html_table = soup.find_all(\"table\")\n",
    "req_table = html_table[1]\n",
    "hrefs_list = req_table.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_href = {\"US\":[],\"Europe\":[],\"Japan\":[],\"AUS_NZ_CANADA\":[],\"Emerging\":[],\"China\":[],\"India\":[],\"Global\":[]}\n",
    "\n",
    "for i in hrefs_list:\n",
    "    name = i.get_text().strip()\n",
    "    try:\n",
    "        href_attr = i['href']\n",
    "        if href_attr.endswith('.xls'):\n",
    "            if \"US\" in name:\n",
    "                req_href[\"US\"].append(href_attr)\n",
    "            elif \"Europe\" in name:\n",
    "                req_href[\"Europe\"].append(href_attr)\n",
    "            elif \"Japan\" in name:\n",
    "                req_href[\"Japan\"].append(href_attr)\n",
    "            elif \"Aus\" in name:\n",
    "                req_href['AUS_NZ_CANADA'].append(href_attr)\n",
    "            elif \"Emerging\" in name:\n",
    "                req_href['Emerging'].append(href_attr)\n",
    "            elif \"China\" in name:\n",
    "                req_href['China'].append(href_attr)\n",
    "            elif \"India\" in name:\n",
    "                req_href['India'].append(href_attr)\n",
    "            elif \"Global\" in name: \n",
    "                req_href['Global'].append(href_attr)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context\n",
    "\n",
    "import os\n",
    "os.makedirs(\"DATA\",exist_ok=True)\n",
    "for country,excel_files in req_href.items():\n",
    "    country_path = os.path.join(\"DATA\",country) \n",
    "    os.makedirs(country_path,exist_ok=True)\n",
    "    for file in excel_files:\n",
    "        file_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        full_file_name = os.path.join(country_path,f\"{file_name}.xls\")\n",
    "        resp = requests.get(file,verify=False)\n",
    "        output = open(full_file_name, 'wb')\n",
    "        output.write(resp.content)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR Emerging WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Europe WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Global WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR AUS_NZ_CANADA WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR China WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR Japan WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n",
      "FOR US WE HAVE DIRECTORY LEN = 24 and ACTUAL LEN = 24\n",
      "FOR India WE HAVE DIRECTORY LEN = 29 and ACTUAL LEN = 29\n"
     ]
    }
   ],
   "source": [
    "for country in os.listdir(\"DATA\"):\n",
    "    dir_len = len(os.listdir(os.path.join(\"DATA\",country)))\n",
    "    country_len = len(req_href[country])\n",
    "    print(f'FOR {country} WE HAVE DIRECTORY LEN = {dir_len} and ACTUAL LEN = {country_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emerging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 86.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 94.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 102.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUS_NZ_CANADA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 104.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 103.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 66.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 102.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 100.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "dir = \"DATA2\"\n",
    "processed_dir = \"Processed Data\"\n",
    "all_infos_dict = []\n",
    "os.makedirs(processed_dir,exist_ok=True)\n",
    "for country in os.listdir(dir):\n",
    "    print(country)\n",
    "    file_name = os.path.join(dir,country)\n",
    "    os.makedirs(os.path.join(processed_dir,country),exist_ok=True)\n",
    "    os.makedirs(file_name,exist_ok=True)\n",
    "    for excel_file in tqdm(os.listdir(file_name)):\n",
    "        full_file_name = os.path.join(file_name,excel_file)\n",
    "        xls = pd.ExcelFile(full_file_name)\n",
    "        sns = xls.sheet_names\n",
    "        for sheet_name in sns:\n",
    "            if \"Var\" in sheet_name or \"var\" in sheet_name:\n",
    "                info_df = xls.parse(sheet_name)\n",
    "                info_df = xls.parse(sheet_name)\n",
    "                info_df.dropna(how=\"all\",inplace=True)\n",
    "                info_dict = {}\n",
    "                for cols in info_df.columns:\n",
    "                    if \"End\" not in cols and 'Unnamed' not in cols:\n",
    "                        info_dict['Summary'] = cols\n",
    "                info_dict['Vars'] = info_df.values[1:].tolist()\n",
    "                all_infos_dict.append(info_dict)\n",
    "            elif \"Industry\" in sheet_name or \"industry\" in sheet_name:\n",
    "                data_df = xls.parse(sheet_name)\n",
    "        try:\n",
    "            data_df.dropna(axis=1,thresh=5,inplace=True)\n",
    "            data_df.dropna(inplace=True)\n",
    "            new_header = data_df.iloc[0] #grab the first row for the header\n",
    "        except:\n",
    "            print(full_file_name)\n",
    "            print(data_df)\n",
    "        data_df = data_df[1:] #take the data less the header row\n",
    "        data_df.reset_index(inplace=True,drop=True)\n",
    "        new_header = [sanitize_column_name(str(col)) for col in new_header]\n",
    "        data_df.columns = new_header #set the header row as the df header\n",
    "        save_name = full_file_name.split(\".\")[0].split(\"/\")[-1]\n",
    "        save_file_path = os.path.join(os.path.join(processed_dir,country),save_name)\n",
    "        data_df.to_csv(save_file_path+\".csv\",index=False)\n",
    "        # with open(save_file_path+\".json\", \"w\") as outfile: \n",
    "        #     json.dump(info_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Summary': 'Measures of accounting returns, to all claim holders (and from operations)',\n",
       " 'Vars': [['Number of firms',\n",
       "   'Number of firms in the industry grouping.',\n",
       "   'Law of large numbers?'],\n",
       "  ['R&D Capitalized',\n",
       "   'My estimate of R&D capitalization, based upon a 5-year straight line amortization period, aggregated across firms in the group',\n",
       "   'Capitalized value of R&D gets added on to book equity and to invested capital'],\n",
       "  ['Capitalized R&D as percent of invested capital',\n",
       "   'My R&D capitalization estimate, as a percent of invested capital including that number, based upon aggregated values across firms',\n",
       "   'Magnitude of investment in R&D, relative to investment in more traditional capital expenditures.'],\n",
       "  ['R&D -LTM',\n",
       "   'Aggregated R&D expenses across the last twelve months, across companies in the group.',\n",
       "   'Spending on R&D in most recent year'],\n",
       "  ['R&D: Years minus 1 to minus 5',\n",
       "   'Aggregated R&D expenses for each of the previous five years, across companies in the group.',\n",
       "   'Sepnding on R&D over last five years']]}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_infos_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry_Name</th>\n",
       "      <th>Number_of_Firms</th>\n",
       "      <th>Capital_Expenditures_(US_$_millions)</th>\n",
       "      <th>Depreciation_&amp;_Amort_((US_$_millions)</th>\n",
       "      <th>Cap_Ex/Deprecn</th>\n",
       "      <th>Acquisitions_(US_$_millions)</th>\n",
       "      <th>Net_R&amp;D_(US_$_millions)</th>\n",
       "      <th>Net_Cap_Ex/Sales</th>\n",
       "      <th>Net_Cap_Ex/_EBIT_(1-t)</th>\n",
       "      <th>Sales/_Invested_Capital_(LTM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Advertising</td>\n",
       "      <td>57</td>\n",
       "      <td>775.729</td>\n",
       "      <td>1887.338</td>\n",
       "      <td>0.411018</td>\n",
       "      <td>322.559</td>\n",
       "      <td>75.0800</td>\n",
       "      <td>-0.016886</td>\n",
       "      <td>-0.218120</td>\n",
       "      <td>3.283403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aerospace/Defense</td>\n",
       "      <td>70</td>\n",
       "      <td>10982.128</td>\n",
       "      <td>13311.598</td>\n",
       "      <td>0.825004</td>\n",
       "      <td>10344.750</td>\n",
       "      <td>830.4634</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.318077</td>\n",
       "      <td>1.984340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air Transport</td>\n",
       "      <td>25</td>\n",
       "      <td>25559.725</td>\n",
       "      <td>10609.948</td>\n",
       "      <td>2.409034</td>\n",
       "      <td>368.210</td>\n",
       "      <td>73.5486</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>1.355173</td>\n",
       "      <td>1.777320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apparel</td>\n",
       "      <td>38</td>\n",
       "      <td>1730.981</td>\n",
       "      <td>1386.480</td>\n",
       "      <td>1.248472</td>\n",
       "      <td>38.739</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.072557</td>\n",
       "      <td>1.773076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auto &amp; Truck</td>\n",
       "      <td>34</td>\n",
       "      <td>29899.486</td>\n",
       "      <td>18677.668</td>\n",
       "      <td>1.600815</td>\n",
       "      <td>193.220</td>\n",
       "      <td>983.0696</td>\n",
       "      <td>0.026577</td>\n",
       "      <td>0.675918</td>\n",
       "      <td>1.048732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Industry_Name  Number_of_Firms  Capital_Expenditures_(US_$_millions)  \\\n",
       "0        Advertising               57                               775.729   \n",
       "1  Aerospace/Defense               70                             10982.128   \n",
       "2      Air Transport               25                             25559.725   \n",
       "3            Apparel               38                              1730.981   \n",
       "4       Auto & Truck               34                             29899.486   \n",
       "\n",
       "   Depreciation_&_Amort_((US_$_millions)  Cap_Ex/Deprecn  \\\n",
       "0                               1887.338        0.411018   \n",
       "1                              13311.598        0.825004   \n",
       "2                              10609.948        2.409034   \n",
       "3                               1386.480        1.248472   \n",
       "4                              18677.668        1.600815   \n",
       "\n",
       "   Acquisitions_(US_$_millions)  Net_R&D_(US_$_millions)  Net_Cap_Ex/Sales  \\\n",
       "0                       322.559                  75.0800         -0.016886   \n",
       "1                     10344.750                 830.4634          0.022829   \n",
       "2                       368.210                  73.5486          0.067203   \n",
       "3                        38.739                   0.9474          0.005365   \n",
       "4                       193.220                 983.0696          0.026577   \n",
       "\n",
       "   Net_Cap_Ex/_EBIT_(1-t)  Sales/_Invested_Capital_(LTM)  \n",
       "0               -0.218120                       3.283403  \n",
       "1                0.318077                       1.984340  \n",
       "2                1.355173                       1.777320  \n",
       "3                0.072557                       1.773076  \n",
       "4                0.675918                       1.048732  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Processed Data/US/capex.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD TABLE NAMES AND METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',Industry_Name,Number_of_Firms,Capital_Expenditures_(US_$_millions),Depreciation_&_Amort_((US_$_millions),Cap_Ex/Deprecn,Acquisitions_(US_$_millions),Net_R&D_(US_$_millions),Net_Cap_Ex/Sales,Net_Cap_Ex/_EBIT_(1-t),Sales/_Invested_Capital_(LTM)\\n0,Advertising,57,775.729,1887.338,0.4110175283918408,322.559,75.07999999999993,-0.0168860862168837,-0.2181203933865563,3.283403041333076\\n1,Aerospace/Defense,70,10982.128,13311.597999999998,0.8250044810547916,10344.749999999998,830.4633999999933,0.0228292321038578,0.318077061754813,1.9843395804666923\\n2,Air Transport,25,25559.725,10609.948,2.4090339556800844,368.21,73.5486000000019,0.0672027546276203,1.3551729988499104,1.7773199625336142\\n3,Apparel,38,1730.9809999999998,1386.4799999999996,1.2484716692631703,38.739,0.9473999999991064,0.0053650838666078,0.0725567415076527,1.7730762352050575\\n4,Auto & Truck,34,29899.486,18677.668,1.6008147269776931,193.22,983.0695999999988,0.0265766732899557,0.6759175844836917,1.048731678622114\\n5,Auto Parts,39,3565.4210000000007,2034.471,1.7525051966825778,1061.7600000000002,454.9352000000008,0.0316936036463534,0.8398185079860373,2.004790963758861\\n6,Beverage (Alcoholic),19,2244.8160000000007,864.0629999999999,2.59797723082692,1370.0,0.0,0.0939030584900288,0.6367443614850201,0.8968231153352131\\n7,Beverage (Soft),29,7926.687000000001,4543.389,1.7446639501922463,674.6669999999999,7.705600000001141,0.0238699281532688,0.1517545513863501,1.6931990505110932\\n8,Broadcasting,22,1837.548,3387.6400000000003,0.5424271764414165,43.856,0.2399999999997817,-0.0206121524427381,-0.2104150803584717,1.1159893806472496\\n9,Brokerage & Investment Banking,27,7783.945999999999,4646.382,1.675270350134793,876.0999999999999,-189.38079999999945,0.0167661627913481,3.4298253908840737,0.2794992243703733\\n'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=250)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "class SQLTableMetadata(dspy.Signature):\n",
    "    \"\"\"Give a suitable table name and description about the given table\"\"\"\n",
    "    pandas_dataframe_str = dspy.InputField(desc=\"First 10 rows of a pandas dataframe delimited by newline character\")\n",
    "    table_name = dspy.OutputField(desc=\"suitable table name\")\n",
    "    table_summary = dspy.OutputField(desc=\"a summary about the table\")\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(SQLTableMetadata)\n",
    "    \n",
    "    def forward(self, pandas_dataframe_str):\n",
    "        return self.prog(pandas_dataframe_str=pandas_dataframe_str)\n",
    "\n",
    "cot = CoT()\n",
    "\n",
    "# cot(pandas_dataframe_str = df.head(10).to_csv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [08:18<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [12:00<00:00, 12.43s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_dir = \"Processed Data\"\n",
    "\n",
    "for country in os.listdir(processed_dir):\n",
    "    country_folder = os.path.join(processed_dir,country)\n",
    "    print(f\"{country}\")\n",
    "    for files in tqdm(os.listdir(country_folder)):\n",
    "        # print(files)\n",
    "        if files.endswith(\".csv\"):\n",
    "            file_name = files.split(\".\")[0]\n",
    "            csv_file_path = os.path.join(country_folder,files)\n",
    "            df = pd.read_csv(csv_file_path,index_col=False)\n",
    "            json_file_path = os.path.join(country_folder,f\"{file_name}.json\")\n",
    "            with open(json_file_path,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            if 'table_name' in data and 'table_summary' in data:\n",
    "                # if data['table_name'] == \"\" or data['table_summary'] == \"\":\n",
    "                if data['table_summary'] == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "            table_preds = cot(pandas_dataframe_str = df.head(10).to_csv())\n",
    "            data['table_name'] = table_preds.table_name\n",
    "            data['table_summary'] = table_preds.table_summary\n",
    "            with open(json_file_path,'w') as f:\n",
    "                json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT TASKS\n",
    "1. Build database with each region for each table\n",
    "2. Embed the table summary and table SCHEMA. Also, embed the table rows\n",
    "3. Retrieval at table level and embed the rows to retrieve relevant rows from the retrieved schema of table\n",
    "4. Final LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATABASE FOR US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir  = \"Processed Data\"\n",
    "def sqlalchemy_engine(region:str):\n",
    "    \"\"\"Create a SQLAlchemy engine for the given region\"\"\"\n",
    "    assert region in os.listdir(processed_dir), f\"{region} is not a valid region from {os.listdir(processed_dir)}\"\n",
    "    engine = create_engine(f\"sqlite:///{region}.db\")\n",
    "    metadata_obj = MetaData()\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    dfs = []\n",
    "    for dataframes_path in os.listdir(region_path):\n",
    "        if dataframes_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(region_path,dataframes_path),index_col=False)\n",
    "            dfs.append((dataframes_path,df))\n",
    "    pbar = tqdm(total=len(dfs),desc=f\"Creating tables for {region}\")\n",
    "    for _, df_table_name in enumerate(dfs):\n",
    "        table_name = df_table_name[0]\n",
    "        table_name = table_name.split(\".\")[0]\n",
    "        df = df_table_name[1]\n",
    "        # print(f\"Creating table: {table_name}\")\n",
    "        create_table_from_dataframe(df,table_name, engine, metadata_obj)\n",
    "        # print(f\"Done creating table for: {table_name}\")\n",
    "        pbar.update(1)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tables for US: 100%|██████████| 24/24 [00:00<00:00, 28.81it/s]\n"
     ]
    }
   ],
   "source": [
    "us_engine = sqlalchemy_engine(\"US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tables for India: 100%|██████████| 29/29 [00:00<00:00, 35.12it/s]\n",
      "Creating tables for China: 100%|██████████| 29/29 [00:00<00:00, 30.45it/s]\n",
      "Creating tables for Europe: 100%|██████████| 29/29 [00:01<00:00, 28.91it/s]\n",
      "Creating tables for Global: 100%|██████████| 29/29 [00:00<00:00, 31.37it/s]\n",
      "Creating tables for AUS_NZ_CANADA: 100%|██████████| 29/29 [00:00<00:00, 30.35it/s]\n",
      "Creating tables for Japan: 100%|██████████| 29/29 [00:00<00:00, 34.07it/s]\n",
      "Creating tables for Emerging: 100%|██████████| 29/29 [00:00<00:00, 33.23it/s]\n"
     ]
    }
   ],
   "source": [
    "india_engine = sqlalchemy_engine(\"India\")\n",
    "china_engine = sqlalchemy_engine(\"China\")\n",
    "europe_engine = sqlalchemy_engine(\"Europe\")\n",
    "global_engine = sqlalchemy_engine(\"Global\")\n",
    "aus_nz_canada_engine = sqlalchemy_engine(\"AUS_NZ_CANADA\")\n",
    "japan_engine = sqlalchemy_engine(\"Japan\")\n",
    "emerging_engine = sqlalchemy_engine(\"Emerging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_infos(sql_engine:sqlalchemy.engine.base.Engine,region:str):\n",
    "    \"\"\"Get all tables in the database\"\"\"\n",
    "    inspector = inspect(sql_engine)\n",
    "    table_names = inspector.get_table_names()\n",
    "    table_infos_dict = {tb: [] for tb in table_names}\n",
    "    for tb in table_names:\n",
    "        column_dict = inspector.get_columns(tb)\n",
    "        schema_str = \"\"\n",
    "        primary_keys = []\n",
    "        for col in column_dict:\n",
    "            schema_str += f\"{col['name']} ({col['type']}), \"\n",
    "            if col[\"primary_key\"] not in primary_keys:\n",
    "                primary_keys.append(col[\"name\"])\n",
    "        with open(os.path.join(processed_dir,region,f\"{tb}.json\")) as f:\n",
    "            table_info = json.loads(f.read())\n",
    "        table_infos_dict[tb] = [\n",
    "            {\n",
    "                \"table_info\": f\"Table {tb} has columns: {schema_str[:-2]}\",\n",
    "                \"table_summary\": f'{table_info[\"Summary\"]}. {table_info[\"table_summary\"]}',\n",
    "            }\n",
    "        ]\n",
    "    return table_infos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_tb_dict = get_table_infos(us_engine,\"US\")\n",
    "india_tb_dict = get_table_infos(india_engine,\"India\")\n",
    "china_tb_dict = get_table_infos(china_engine,\"China\")\n",
    "europe_tb_dict = get_table_infos(europe_engine,\"Europe\")\n",
    "global_tb_dict = get_table_infos(global_engine,\"Global\")\n",
    "aus_nz_canada_tb_dict = get_table_infos(aus_nz_canada_engine,\"AUS_NZ_CANADA\")\n",
    "japan_tb_dict = get_table_infos(japan_engine,\"Japan\")\n",
    "emerging_tb_dict = get_table_infos(emerging_engine,\"Emerging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS\n",
    "\n",
    "1. Embed the table summary and table SCHEMA to get the table that the user is looking for\n",
    "2. Embed the table rows for each table, so as to get relevant rows from the retrieved table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBED THE TABLE SUMMARY AND TABLE SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "import uuid\n",
    "\n",
    "load_dotenv(override=True)\n",
    "emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.environ['OPENAI_API_KEY'],\n",
    "                model_name=\"text-embedding-3-small\")\n",
    "def embed_table_info(region:str,tb_dict):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "\n",
    "    table_collection = client.get_or_create_collection(name=\"table\",embedding_function=emb_fn)\n",
    "\n",
    "    table_docs = []\n",
    "    table_metadata = []\n",
    "\n",
    "\n",
    "    for table_name,table_data in tb_dict.items():\n",
    "        table_docs.append(table_data[0]['table_info'] + \". \" + table_data[0]['table_summary'])\n",
    "        table_metadata.append({\"table_name\":table_name})\n",
    "    table_ids = [f\"id{i}\" for i in range(len(table_docs))]\n",
    "    assert len(table_docs) == len(table_metadata)\n",
    "\n",
    "    batches = create_batches(api=client,ids=table_ids, documents=table_docs, metadatas=table_metadata)\n",
    "    for batch in tqdm(batches,desc=\"Embedding table info\"):\n",
    "        # print(f\"Adding batch of size {len(batch[0])}\")\n",
    "        table_collection.add(ids=batch[0],\n",
    "                    documents=batch[3],\n",
    "                    metadatas=batch[2])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"US\"\n",
    "def embed_rows(region:str):\n",
    "    client = chromadb.PersistentClient(path=f\"{region}_TABLE\")\n",
    "\n",
    "    rows_collection = client.get_or_create_collection(name=\"rows\",embedding_function=emb_fn)\n",
    "\n",
    "    rows_docs = []\n",
    "    rows_metadata = []\n",
    "    region_path = os.path.join(processed_dir,region)\n",
    "    for df_path in os.listdir(region_path):\n",
    "        df_full_path = os.path.join(region_path,df_path)\n",
    "        df = pd.read_csv(df_full_path,index_col=False)\n",
    "        for idx,row in df.iterrows():\n",
    "            row_str = \"\"\n",
    "            for rv in row.values:\n",
    "                row_str+=str(rv) + \", \"\n",
    "            rows_docs.append(row_str[:-2])\n",
    "            rows_metadata.append({\"table_name\":df_path.split(\".\")[0],\"region\":region,\"index\":idx})\n",
    "    row_ids = [f\"id{i}\" for i in range(len(rows_docs))]\n",
    "    assert len(rows_docs) == len(rows_metadata)\n",
    "\n",
    "    batches = create_batches(api=client,ids=row_ids, documents=rows_docs, metadatas=rows_metadata)\n",
    "    for batch in tqdm(batches,desc=\"Embedding rows\"):\n",
    "        # print(f\"Adding batch of size {len(batch[0])}\")\n",
    "        rows_collection.add(ids=batch[0],\n",
    "                        documents=batch[3],\n",
    "                        metadatas=batch[2])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding table info:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "region = \"US\"\n",
    "embed_table_info(region,us_tb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding rows:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membed_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36membed_rows\u001b[0;34m(region)\u001b[0m\n\u001b[1;32m     22\u001b[0m batches \u001b[38;5;241m=\u001b[39m create_batches(api\u001b[38;5;241m=\u001b[39mclient,ids\u001b[38;5;241m=\u001b[39mrow_ids, documents\u001b[38;5;241m=\u001b[39mrows_docs, metadatas\u001b[38;5;241m=\u001b[39mrows_metadata)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(batches,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding rows\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# print(f\"Adding batch of size {len(batch[0])}\")\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mrows_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:154\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/chromadb/api/types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:188\u001b[0m, in \u001b[0;36mOpenAIEmbeddingFunction.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Call the OpenAI Embedding API\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v1:\n\u001b[0;32m--> 188\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deployment_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Sort resulting embeddings by index\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     sorted_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(embeddings, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: e\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/openai/resources/embeddings.py:113\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/openai/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/openai/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSPy-contributions/Text-to-SQL/.venv/lib/python3.10/site-packages/openai/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "embed_rows(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
